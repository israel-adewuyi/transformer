{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXHQbTNkAZxv4wkCJ6UYXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israel-adewuyi/transformer/blob/main/attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyeGtsxNjmbx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Embedding vector of N * d_model, i.e N words/tokens in the sequence, residing in\n",
        "    a d_model-dimensional space.\n",
        "\n",
        "\"\"\"\n",
        "d_model = 8\n",
        "d_k = 4\n",
        "N = 4 # sequence length\n",
        "batch_size = 1\n",
        "heads = 2"
      ],
      "metadata": {
        "id": "zqFTLZbijtz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self *Attention*"
      ],
      "metadata": {
        "id": "tpextQcHvuB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K,Q, V  - shape = d_model by d_k\n",
        "K, Q, V= torch.rand((d_model, d_k)), torch.rand(d_model, d_k), torch.rand(d_model, d_k)"
      ],
      "metadata": {
        "id": "f4LCC--LkChs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask is the same dim as the attention pattern matrix - d_model * d_model\n",
        "mask = torch.ones(d_model, d_model)\n",
        "mask = torch.tril(mask)\n",
        "mask[mask == 0] = -float('inf')\n",
        "mask[mask == 1] = 0"
      ],
      "metadata": {
        "id": "G_1I4-QVqCdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = ((Q @ K.T) / np.sqrt(d_k)) + mask\n",
        "Z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nToNn2etmpNP",
        "outputId": "787e1a4e-e3bd-4c6e-a82e-c1507e65c0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3811,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.4749, 0.0926,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.7087, 0.2542, 0.5864,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.5527, 0.2629, 0.3430, 0.3673,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.7194, 0.4565, 0.5250, 0.5358, 0.3033,   -inf,   -inf,   -inf],\n",
              "        [0.5079, 0.1389, 0.5416, 0.3255, 0.2208, 0.4135,   -inf,   -inf],\n",
              "        [0.5179, 0.2049, 0.3409, 0.3261, 0.1632, 0.2434, 0.3883,   -inf],\n",
              "        [0.3646, 0.2434, 0.1226, 0.2567, 0.1531, 0.2558, 0.4314, 0.4805]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z_mask = F.softmax(Z, dim=-1)\n",
        "Z_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AstLQZ_dostX",
        "outputId": "c4081b43-2e2a-4cf8-d2fb-8da13643a845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = Z_mask @ V\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fSjf_mVsNwi",
        "outputId": "f694adf5-63f0-4141-b78a-86d68dafebd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9856, 0.6100, 0.5396, 0.9817],\n",
              "        [0.9688, 0.5313, 0.3871, 0.7837],\n",
              "        [0.6311, 0.4680, 0.2634, 0.6950],\n",
              "        [0.6687, 0.4565, 0.2728, 0.6136],\n",
              "        [0.5776, 0.4102, 0.3798, 0.6209],\n",
              "        [0.5661, 0.3990, 0.3281, 0.5517],\n",
              "        [0.6352, 0.4415, 0.3763, 0.5837],\n",
              "        [0.5927, 0.4746, 0.4344, 0.5776]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together"
      ],
      "metadata": {
        "id": "mrntFX0ssi4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(K, Q, V, is_mask_present):\n",
        "    mask = torch.tril(torch.ones(d_model, d_model))\n",
        "    mask[mask == 0] = -float('inf')\n",
        "    mask[mask == 1] = 0\n",
        "    attention_pattern = (Q @ K.T) / np.sqrt(d_k)\n",
        "\n",
        "    if is_mask_present == True:\n",
        "        attention_pattern += mask\n",
        "\n",
        "    Z = F.softmax(attention_pattern, dim=-1) @ V\n",
        "    return Z"
      ],
      "metadata": {
        "id": "2syexdT7sfVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(self_attention(K, Q, V, is_mask_present=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TFaKqqatq3A",
        "outputId": "a0393e64-dd5e-4e43-e8c0-0189157b475a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9856, 0.6100, 0.5396, 0.9817],\n",
            "        [0.9688, 0.5313, 0.3871, 0.7837],\n",
            "        [0.6311, 0.4680, 0.2634, 0.6950],\n",
            "        [0.6687, 0.4565, 0.2728, 0.6136],\n",
            "        [0.5776, 0.4102, 0.3798, 0.6209],\n",
            "        [0.5661, 0.3990, 0.3281, 0.5517],\n",
            "        [0.6352, 0.4415, 0.3763, 0.5837],\n",
            "        [0.5927, 0.4746, 0.4344, 0.5776]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "xRrvgBX3wAwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand((batch_size, N, d_model))\n",
        "X, X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIV5ut4dwiHE",
        "outputId": "ea6bd2de-d179-4da6-8dd7-a72ed8e11ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.3271, 0.7711, 0.1463, 0.7642, 0.5292, 0.4177, 0.9999, 0.5407],\n",
              "          [0.4655, 0.9250, 0.5578, 0.8686, 0.8262, 0.7051, 0.6217, 0.1012],\n",
              "          [0.8937, 0.7151, 0.2110, 0.1878, 0.2648, 0.0572, 0.6349, 0.0100],\n",
              "          [0.6035, 0.3324, 0.1408, 0.9069, 0.8992, 0.7332, 0.4593, 0.1390]]]),\n",
              " torch.Size([1, 4, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the linear layer to accommodate multiple heads\n",
        "qkv_layer = torch.nn.Linear(d_model, 3 * heads * d_k)\n",
        "qkv_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTWDicxqBzwY",
        "outputId": "b89db91f-ef35-4379-a828-495e7f5bd7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=8, out_features=24, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = qkv_layer(X)\n",
        "qkv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-EqaRNwCPMn",
        "outputId": "64191ae4-f199-43c6-9928-686b54506317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    For each batch, for each input word/token in the sequence, for each attention\n",
        "        head, there are q, k and v matrices with dims of d_k\n",
        "\"\"\"\n",
        "qkv = qkv.reshape(batch_size, N, heads, 3 * d_k)"
      ],
      "metadata": {
        "id": "EW5M1yk1FcIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bscphbOOF4qd",
        "outputId": "89745d0f-3dd5-4aa0-e07d-c3e98e51db1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 2, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q, k, v = qkv.chunk(3, dim=-1)"
      ],
      "metadata": {
        "id": "xmZWRnpqF7Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buf-iUEWHCMB",
        "outputId": "f2dc299d-0fa3-4e52-c57d-d6915769007b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def single_head_attention(Q, K, V):\n",
        "    scores = (Q @ K.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "    attention_pattern = F.softmax(scores, dim=-1)\n",
        "    Z = attention_pattern @ V\n",
        "    return Z"
      ],
      "metadata": {
        "id": "rmli9igDGMOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to apply this function to each head's Q, K, V\n",
        "outputs = []\n",
        "for i in range(heads):\n",
        "    out = single_head_attention(q[:, :, i], k[:, :, i], v[:, :, i])\n",
        "    outputs.append(out)"
      ],
      "metadata": {
        "id": "QrM-GgfvG6iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04HRnMJAHMJ8",
        "outputId": "b24200e2-513b-471f-f905-7068ce7dc4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[-0.0820,  0.5960, -0.6383,  0.1604],\n",
              "          [-0.0837,  0.5961, -0.6411,  0.1603],\n",
              "          [-0.0815,  0.5950, -0.6378,  0.1599],\n",
              "          [-0.0839,  0.5955, -0.6417,  0.1591]]], grad_fn=<UnsafeViewBackward0>),\n",
              " tensor([[[ 0.6695, -0.6061, -0.3127, -0.0378],\n",
              "          [ 0.6654, -0.6082, -0.3121, -0.0362],\n",
              "          [ 0.6689, -0.6024, -0.3109, -0.0373],\n",
              "          [ 0.6635, -0.6063, -0.3105, -0.0353]]], grad_fn=<UnsafeViewBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it together"
      ],
      "metadata": {
        "id": "QguHUaWkIm1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0EcWWqssIp56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}